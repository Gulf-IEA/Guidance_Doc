[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guidance Document",
    "section": "",
    "text": "Preface\n\n\n\n\n\nThis is the guidance document compiled by members of the Gulf of Mexico Integrated Ecosystem Assessment members to facilitate program missions and promote open science."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Tutorials/Tutorial_Intro.html",
    "href": "Tutorials/Tutorial_Intro.html",
    "title": "Tutorials",
    "section": "",
    "text": "Relevant Tutorials from Seaside Chats\nWe can put relevant leading text here for a part if necessary."
  },
  {
    "objectID": "Tutorials/Creating_a_Quarto_Book.html#creating-a-new-quarto-book-repo",
    "href": "Tutorials/Creating_a_Quarto_Book.html#creating-a-new-quarto-book-repo",
    "title": "2  Creating a Quarto Book",
    "section": "2.1 Creating a new Quarto Book & Repo",
    "text": "2.1 Creating a new Quarto Book & Repo\nThere are several ways to do this which are well outlined in chapters 15, 16, and 17 of Happy Git and GitHub for the useR.\nTo be able to use the Quarto Book template when creating a new project, so that all the necessary files are included I followed these steps.\n\nCreate a New Project in RStudio\nChoose “New Directory” to see a list of possible formats\n\nSelect “Quarto Book”\nEnter the name of the project (will eventually become your repo name) and make sure it is in the correct directory.\nClick “Create a git repository” before making the project.\n\nStage and commit the files the files that are part of the Quarto Book template (can alter later)\nCreate a repo on GitHub by running usethis::use_github() in the console\n\nTo add the quarto book to the Gulf IEA organisation I added the organisation=“Gulf-IEA” argument\nIf you have already used RStudio with GitHub it will know your account and should know where to go\n\nThis create a new repo on GitHub as the origin and opens the repo in your browser"
  },
  {
    "objectID": "Tutorials/Adapt NE SoE Indicator.html#steps-when-first-becoming-acquainted",
    "href": "Tutorials/Adapt NE SoE Indicator.html#steps-when-first-becoming-acquainted",
    "title": "3  Adapt NE SoE Indicator",
    "section": "3.1 Steps when first becoming acquainted",
    "text": "3.1 Steps when first becoming acquainted\n\n3.1.1 1. View NE SoE report to determine indicator to replicate\nNE State of the Ecosystem Report 2022\n\n\n\nIndicator from the NE SoE 2022 report that I wished to duplicate for the GoM.\n\n\n\n\n3.1.2 2. Go to GitHub site of the group that creates the NE SoE report to search for code\n\nSearched “GitHub NOAA NE SoE” and found the “Ecosystem Dynamics and Assessment Branch” GitHub (EDAB GitHub)\nTheir GitHub hosts the repo “SOE-NEFMC” where the NE SoE report is housed\n\n\n\nScreenshot of the EDAB Github. The group that creates the NE SoE Report.\n\n\nInside the “SOE-NEFMC” repo there are many files. I know that “.Rmd” and “.Qmd” are Rmardown and Quarto file extensions, respectively. These file types are used to create reports, so I knew that this was the code that was used to create the SoE report.\n\n\n\nThe .Rmd file that creates the NE SoE report.\n\n\nI searched through the .Rmd document code using Ctrl+F from text in the report to find where the code for the indicator is.\n\n\n\nTop: Copied text from NE SoE Report. Bottom: Corresponding text in code.\n\n\nThis gave me the name of the indicator throughout their code is “seasonal-sst-anom-gridded” and gave me more information for my search on the actual code to analyze and plot the data.\n\n\n\n3.1.3 3. Search for the “seasonal-sst-anom-gridded” code in the ecodata repository (still in the EDAB GitHub)\n\nWe learned when Kim Bastille spoke to us that the “ecodata” package was created to house their data and the functions that sort, format, analyze, and plot their indicators.\nAs seen earlier, the ecodata repo is on their GitHub and houses all that code.\n\n\n\n\nA screenshot of the ecodata repo and the choices of folders I had to search through.\n\n\n\nI first went to the “chunk-scripts” folder because Kim had mentioned that this is created using the ‘readLines’ function to take the script “chunks” for a specific indicator and puts them in this folder. In the folder I found a list of what appears to be all separate indicators and thought this would have all of the code I needed.\n\n\n\nList of indicators in the “script-chunks” folder in the ecodata repo.\n\n\nI installed the ecodata package that contains the necessary data and functions to run the code that is in that “LTL_MAB.Rmd-seasonal-sst-anom-gridded.R” file.\nHowever, ran I copy and pasted the code and ran it locally it would not work. I kept getting “Error: x.shade.min” not found. Letting me know that I was missing some piece of the puzzle.\nI continued to search through the ecodata repo to find other locations where the code for the “seasonal-sst-anom-gridded” indicator may be\nI found a docs folder that had LTL files which matched with the “LTL” in the indicator name of the chunks-script and looked there\n\n\n\nThe HTML document I looked at for more information on the indicator I was searching for.\n\n\nThere is a .Rmd and .html file. The .Rmd file (RMarkdown) file creates the HTML file. I decided to first view the HTML file because it was easier to see the indicators, then used the .Rmd to view the code.\n\n\n\n\n\n\n\nPreviewing HTML files from GitHub\n\n\n\nTIP: If you just click the .html file you will see the raw HTML code and not the website. However you can preview the rendered HTML website by pasting this text “http://htmlpreview.github.io/?” before the url to the LTL_MAB.html document.\n-   Preview Text + .html URL = Website preview\n-   Preview Text = http://htmlpreview.github.io/?\n-   HTML Text = https://github.com/NOAA-EDAB/ecodata/blob/master/docs/LTL_MAB.html\n-   Preview Link = http://htmlpreview.github.io/?https://github.com/NOAA-EDAB/ecodata/blob/master/docs/LTL_MAB.html\n\n\n\n\n\nThe raw HTML code after clicking on the “LTL_MAB.html” in the docs repo. The URL at the top is what you add onto the preview code mentioned in text.\n\n\n\nThis document once again shows the indicator that I was looking for, so I looked at the doc for any additional code that I may have been missing from my first try.\nTo look at the code I looked at the .Rmd version of that document. Reminder the .Rmd holds the code to run the analysis and the .html holds the code to put it together in a document.\nWhen I was scrolling through the .Rmd document looking for the chunk addressing SST anomaly, I found code addressing the missing variable that was part of the previous error messages “x.shade.min”. It was part of a setup code chunk in the beginning of the document which was not included in the previous code I explored.\n\n\n\nSetup code chunk at the beginning of a document that helps define some variables for several indicators throughout the document.\n\n\nFor this indicator I needed that code chunk so I copy/pasted this chunk before the previous code chunk and tried to recreate the indicator from the report.\n\n\n\n\n\n\n\nReplicate First\n\n\n\n\nTIP: When adapting/manipulating code I recommend first reproducing it as is to make sure the code is working and then making changes to it as you need.\n\n\n\n\nCombining the setup code and the indicator code gave me this.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ecodata)\nlibrary(sf)\n\n### SETUP CODE FROM LTL_MAB.RmD\n# Set lat/lon window for maps\nxmin = -77\nxmax = -65\nymin = 36\nymax = 45\nxlims &lt;- c(xmin, xmax)\nylims &lt;- c(ymin, ymax)\n#Time series constants\nshade.alpha &lt;- 0.3\nshade.fill &lt;- \"lightgrey\"\nlwd &lt;- 1\npcex &lt;- 2\ntrend.alpha &lt;- 0.5\ntrend.size &lt;- 2\nhline.size &lt;- 1\nhline.alpha &lt;- 0.35\nhline.lty &lt;- \"dashed\"\nlabel.size &lt;- 5\nhjust.label &lt;- 1.5\nletter_size &lt;- 4\nfeeding.guilds &lt;- c(\"Apex Predator\",\"Piscivore\",\"Planktivore\",\"Benthivore\",\"Benthos\")\nx.shade.min &lt;- 2012\nx.shade.max &lt;- 2022\n#Function for custom ggplot facet labels\nlabel &lt;- function(variable,value){\n  return(facet_names[value])\n}\n\n### ORIGINAL CODE FROM \"LTL_MAB.Rmd-seasonal-sst-anom-gridded.R\" IN SCRIPT-CHUNKS \n#EPU shapefile\n# ne_epu_sf &lt;- ecodata::epu_sf %&gt;%  dplyr::filter(EPU %in% c(\"GOM\",\"GB\"))\nne_epu_sf &lt;- ecodata::epu_sf[4,4] #MAB SET\n\n# View(ecodata::epu_sf)\n\n#Map line parameters\nmap.lwd &lt;- 0.4\n# Set lat/lon window for maps\nxmin = -73\nxmax = -65\nymin = 39\nymax = 45\nxlims &lt;- c(xmin, xmax)\nylims &lt;- c(ymin, ymax)\nsst &lt;- ecodata::seasonal_sst_anomaly_gridded  #WHERE YOU LOAD THE DATA FROM ECODATA\nsst$Season &lt;- factor(sst$Season, levels = c(\"Winter\",\n                                            \"Spring\",\n                                            \"Summer\",\n                                            \"Fall\"))\nsst&lt;- sst %&gt;% dplyr::mutate(Value = replace(Value, Value &gt; 5, 5))\nsst_map &lt;- \n  ggplot2::ggplot() +\n  ggplot2::geom_tile(data = sst, aes(x = Longitude, y = Latitude,fill = Value)) +\n  ggplot2::geom_sf(data = ecodata::coast, size = map.lwd) +\n  ggplot2::geom_sf(data = ne_epu_sf, fill = \"transparent\", size = map.lwd) +\n  ggplot2::scale_fill_gradient2(name = \"Temp./nAnomaly (C)\",\n                       low = scales::muted(\"blue\"),\n                       mid = \"white\",\n                       high = scales::muted(\"red\"),\n                       limits = c(-5,5), \n                       labels = c(\"&lt;-5\", \"-2\", \"0\", \"2\", \"&gt;5\")) +\n  ggplot2::coord_sf(xlim = xlims, ylim = ylims) +\n  ggplot2::facet_wrap(Season~.) +\n  ecodata::theme_map() +\n  ggplot2::ggtitle(\"SST anomaly (2022)\") +\n  ggplot2::xlab(\"Longitude\") +\n  ggplot2::ylab(\"Latitude\") +\n  ggplot2::theme(panel.border = element_rect(colour = \"black\", fill=NA, size=0.75),\n        legend.key = element_blank(),\n        axis.title = element_text(size = 11),\n        strip.background = element_blank(),\n        strip.text=element_text(hjust=0),\n        axis.text = element_text(size = 8), \n        axis.title.y = element_text(angle = 90))+\n  ecodata::theme_title()\n\nsst_map \n\n\n\n\n\n\n\n3.1.4 4. Change Indicator to Gulf of Mexico\n\nNow that I had working code I began examining the code for the pieces that change the geographic area for analyses and plotting to change that to the GoM.\nOf the above code, here are the lines I changed to accommodate to GoM\nThe x and y ranges for figure plots (took the limits of the GoM shapefile I had)\n\n\nCode\n# Set lat/lon window for maps\nxmin = -98.05 #CHANGED\nxmax = -80.43 #CHANGED\nymin = 17.40 #CHANGED\nymax = 31.46 #CHANGED\n\n\nI substituted the shapefile that was provided in the indicator text with a GoM version of it. To do this I double checked that it was in the same format as the one used originally. For this use it needed to be the geometry of a polygon of class sf. To do this I used the function st_as_sf() from sf.\n\n\nCode\n#EPU shapefile\n# ne_epu_sf &lt;- ecodata::epu_sf %&gt;%  dplyr::filter(EPU %in% c(\"GOM\",\"GB\"))\n# ne_epu_sf &lt;- ecodata::epu_sf[4,4]\n\n\n#OUR REGION\nGoM_shp&lt;-rgdal::readOGR(\"C:/Users/brittanytroast/Dropbox/Work/Seaside Chats/Tutorials/Pics/Adapt NE SoE Indic/GoM Shapefile/GoM.shp\")\nGoM_shp_sf&lt;-sf::st_as_sf(GoM_shp)\nGoM_shp_sf&lt;-GoM_shp_sf$geometry\n\n\nThere was a second set of lat/lon ranges that I also changed. This just overrides the above code, but I changed it anyway.\n\n\nCode\n# Set lat/lon window for maps\nxmin = -98.05 #CHANGED\nxmax = -80.43 #CHANGED\nymin = 17.40 #CHANGED\nymax = 31.46 #CHANGED\n\n\nThe last code I found addressing the geographic area of the indicator was in the ggplot code chunk where it calls on the shapefile. Since I have already renamed and loaded in a new shapefile I made sure to swap out the new name (or you could leave it as the same name and skip this step) in the plotting code.\n\n\nCode\n  ggplot2::geom_sf(data = GoM_shp_sf, fill = \"transparent\", size = map.lwd)\n\n\nWith these changes I reran the code hoping to have the new indicator with GoM SST anomalies.\n\n\nCode\n# Set lat/lon window for maps\nxmin = -98.05 #CHANGED\nxmax = -80.43 #CHANGED\nymin = 17.40 #CHANGED\nymax = 31.46 #CHANGED\nxlims &lt;- c(xmin, xmax)\nylims &lt;- c(ymin, ymax)\n#Time series constants\nshade.alpha &lt;- 0.3\nshade.fill &lt;- \"lightgrey\"\nlwd &lt;- 1\npcex &lt;- 2\ntrend.alpha &lt;- 0.5\ntrend.size &lt;- 2\nhline.size &lt;- 1\nhline.alpha &lt;- 0.35\nhline.lty &lt;- \"dashed\"\nlabel.size &lt;- 5\nhjust.label &lt;- 1.5\nletter_size &lt;- 4\nfeeding.guilds &lt;- c(\"Apex Predator\",\"Piscivore\",\"Planktivore\",\"Benthivore\",\"Benthos\")\nx.shade.min &lt;- 2012\nx.shade.max &lt;- 2022\n#Function for custom ggplot facet labels\nlabel &lt;- function(variable,value){\n  return(facet_names[value])\n}\n\n\n#OUR REGION\nGoM_shp&lt;-rgdal::readOGR(\"C:/Users/brittanytroast/Dropbox/Work/Seaside Chats/Tutorials/Pics/Adapt NE SoE Indic/GoM Shapefile/GoM.shp\")\n\n\nOGR data source with driver: ESRI Shapefile \nSource: \"C:\\Users\\brittanytroast\\Dropbox\\Work\\Seaside Chats\\Tutorials\\Pics\\Adapt NE SoE Indic\\GoM Shapefile\\GoM.shp\", layer: \"GoM\"\nwith 1 features\nIt has 10 fields\n\n\nCode\nGoM_shp_sf&lt;-sf::st_as_sf(GoM_shp)\nshp_geom&lt;-GoM_shp_sf$geometry\n\n#Map line parameters\nmap.lwd &lt;- 0.4\n# Set lat/lon window for maps\nxmin = -98.05 #CHANGED\nxmax = -80.43 #CHANGED\nymin = 17.40 #CHANGED\nymax = 31.46 #CHANGED\nxlims &lt;- c(xmin, xmax)\nylims &lt;- c(ymin, ymax)\n\nsst &lt;- ecodata::seasonal_sst_anomaly_gridded  #WHERE YOU LOAD THE DATA FROM ECODATA\nsst$Season &lt;- factor(sst$Season, levels = c(\"Winter\",\n                                            \"Spring\",\n                                            \"Summer\",\n                                            \"Fall\"))\n\nsst&lt;- sst %&gt;% dplyr::mutate(Value = replace(Value, Value &gt; 5, 5))\nsst_map &lt;- \n  ggplot2::ggplot() +\n  ggplot2::geom_tile(data = sst, aes(x = Longitude, y = Latitude,fill = Value)) +\n  ggplot2::geom_sf(data = ecodata::coast, size = map.lwd) +\n  ggplot2::geom_sf(data = GoM_shp_sf, fill = \"transparent\", size = map.lwd) +\n  ggplot2::scale_fill_gradient2(name = \"Temp./nAnomaly (C)\",\n                       low = scales::muted(\"blue\"),\n                       mid = \"white\",\n                       high = scales::muted(\"red\"),\n                       limits = c(-5,5), \n                       labels = c(\"&lt;-5\", \"-2\", \"0\", \"2\", \"&gt;5\")) +\n  ggplot2::coord_sf(xlim = xlims, ylim = ylims) +\n  ggplot2::facet_wrap(Season~.) +\n  ecodata::theme_map() +\n  ggplot2::ggtitle(\"SST anomaly (2022)\") +\n  ggplot2::xlab(\"Longitude\") +\n  ggplot2::ylab(\"Latitude\") +\n  ggplot2::theme(panel.border = element_rect(colour = \"black\", fill=NA, size=0.75),\n        legend.key = element_blank(),\n        axis.title = element_text(size = 11),\n        strip.background = element_blank(),\n        strip.text=element_text(hjust=0),\n        axis.text = element_text(size = 8), \n        axis.title.y = element_text(angle = 90))+\n  ecodata::theme_title()\n\nsst_map \n\n\n\n\n\nAs you can see in the output, we are getting really close with figures of our study area, however, with a glaring lack of actual SST anomaly data.\nFrom here I took a further look at the SST Anomaly that data that came in the ecodata package and looked at the Lat/Long range to figure out what area it covers and realized that the package did not include the data coverage I needed.\n\n\nCode\nhead(ecodata::seasonal_sst_anomaly_gridded)\n\n\n  Latitude Longitude      Value Season\n1   45.875   -63.625 -0.2245555 Winter\n2   45.875   -63.375 -0.1810536 Winter\n3   45.875   -63.125 -0.1650498 Winter\n4   45.875   -62.875 -0.1733716 Winter\n5   45.875   -62.625 -0.2262375 Winter\n6   45.875   -62.375 -0.2597778 Winter\n\n\nCode\nrange(ecodata::seasonal_sst_anomaly_gridded$Latitude)\n\n\n[1] 35.125 45.875\n\n\nCode\nrange(ecodata::seasonal_sst_anomaly_gridded$Longitude)\n\n\n[1] -76.875 -60.125\n\n\nI then went on a hunt to figure out more about where this data came from and happened upon the NE SoE Technical document which describes every indicator and points of contact, details analysis, and gives data sources.\nNE SoE Technical Document\nSpecific Link in Tech Doc to Indicator of Interest\n\n\n\nExcerpt from NE SoE Technical Document on SST Anomaly Data extraction with links to code to do so.\n\n\nThis document gave the data sources needed for the analysis and linked the code they used to analyze and format the data for use with the above code. (The gridded link in this section of the tech doc)\nLink to Code for processing the NOAA OISST SST Anomaly Data\n\n\n\n3.1.5 5. Process New SST Anomaly Data for the GoM\n\nNow we just need to follow the steps in the code to get and process the same data for our specified region.\nWhen examining the code the first thing I noticed was the code saying “here” which linked this code to the ecodata repo\n\nTo get around this I forked the ecodata repo to my computer so that it could access it however this is not necessary\nUpon reviewing the code I realized this was just providing a file directory and since we are providing our own directory we can just ignore those two lines of code (comment them out with a # so that they do not run)\n\n\n\n\n\nCode showing the here::here code\n\n\n\nIn the code there is a chunk warning that you must separately download two data files (with link provided) and alter the code to the directory where you have saved the files.\n\n\n\nPortion of code detailing that you need to locally download the SST data since it is too large to host in the ecodata repo.\n\n\nLink to NOAA OI SST Anomaly data source\n\nDownload the two files as instructed in code\nDOUBLE CHECK that your filepath matches where you have it stored before continuing to next chunk of code. Remove the ltm.dir with your personal file path\n\nI double checked the rest of the code for any spatial things I may need to change and there was none. There was a chunk of code at the end that addressed heatwaves that I did not need and did not run.\nRunning the code produced a dataset of SST anomalies for the GoM in the correct format for using the SoE indicator code.\n\nI have provided the code that I used for this data with the same disclaimer that you must download the data separately.\nPlaces with ALL CAPS comments are what I changed. They are in the extent, naming the file.dir argument, adding file.dir in the file.path() function, and “commenting out” unnecessary code at the end of the script\n\n\n\nCode\n#Processing for spatial SST anomaly\n\nlibrary(dplyr)\nlibrary(raster)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(ncdf4)\nlibrary(reshape2)\n\nrast_prep &lt;- function(r){\n  r &lt;- rotate(r) #Rotate\n  r &lt;- crop(r, extent(-98.05,-80.43,17.40,31.46)) #Crop (CHANGED FOR GOM)\n  return(r)\n}\n\n\ncrs &lt;- \"+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40+lon_0=-77 +x_0=0 +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0\"\n\nseasonal_sst_anomaly_gridded_day_nc &lt;-\"sst.day.mean.2022.nc\"\nseasonal_sst_anomaly_gridded_ltm_nc &lt;- \"sst.day.mean.ltm.1982-2010.nc\"\n\n#These data are large files that are not included among ecodata source files. They are accessible\n#here: https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html\n# but are removed after use as they are too large to store on github\n\nfile.dir&lt;-\"C:/Users/brittanytroast/Documents/GitHub/Testing/Testing/SST Data\" #ADDED TO DIRECT WHERE FILES ARE\n\nsst.2019 &lt;- rast_prep(stack(file.path(file.dir, seasonal_sst_anomaly_gridded_day_nc))) #CHECK TO MAKE SURE FILEPATH IS CORRECT\nltm &lt;- rast_prep(stack(file.path(file.dir, seasonal_sst_anomaly_gridded_ltm_nc))) #CHECK TO MAKE SURE FILEPATH IS CORRECT\n\nwinter.ltm &lt;- ltm[[1:90]]\nspring.ltm &lt;- ltm[[91:181]]\nsummer.ltm &lt;- ltm[[182:273]]\nfall.ltm &lt;- ltm[[274:365]]\n\nwinter.anom &lt;- sst.2019[[1:90]] - winter.ltm\nspring.anom &lt;- sst.2019[[91:181]] - spring.ltm\nsummer.anom &lt;- sst.2019[[182:273]] - summer.ltm\nfall.anom &lt;- sst.2019[[274:342]] - fall.ltm ### switched to cover just dates included in 2022.nc up to Dec 9th.\n\n\nrast_process &lt;- function(r, season){\n  r &lt;- raster::stackApply(r, indices = rep(1,nlayers(r)),mean) #Find mean anomaly\n  crs(r) &lt;- crs #Add SOE CRS\n  ### Remove smoothing steps due to \"over smoothing'\n  #r &lt;- disaggregate(r, 5) #interpolate step 1 - create higher res grid\n  #r &lt;- focal(r, w=matrix(1,nrow=5,ncol=5), fun=mean,\n  #           na.rm=TRUE, pad=TRUE) #interpolate step 2 - moving window\n  r &lt;- as(r, \"SpatialPointsDataFrame\") #Convert to ggplot-able object\n  r &lt;- as.data.frame(r)\n  r &lt;- r %&gt;%\n    reshape2::melt(id = c(\"y\",\"x\")) %&gt;%\n    dplyr::rename(Latitude = y, Longitude = x) %&gt;%\n    dplyr::select(-variable) %&gt;%\n    dplyr::mutate(Season = season) %&gt;%\n    dplyr::rename(Value = value)\n\n  return(r)\n}\n\n#CHANGED NAME TO _GOM SO IT WASNT THE SAME AS BEFORE\nseasonal_sst_anomaly_gridded_GoM &lt;-\n  rbind(rast_process(winter.anom,season = \"Winter\"),\n      rast_process(spring.anom,season = \"Spring\"),\n      rast_process(summer.anom, season = \"Summer\"),\n      rast_process(fall.anom, season = \"Fall\")) %&gt;%\n  tibble::as_tibble()\n\n#Don't need... here to write metadata\n# # metadata ----\n# attr(seasonal_sst_anomaly_gridded, \"tech-doc_url\") &lt;- \"https://noaa-edab.github.io/tech-doc/seasonal-sst-anomalies.html\"\n# attr(seasonal_sst_anomaly_gridded, \"data_files\")   &lt;- list(\n#   seasonal_sst_anomaly_gridded_day_nc = seasonal_sst_anomaly_gridded_day_nc,\n#   seasonal_sst_anomaly_gridded_ltm_nc = seasonal_sst_anomaly_gridded_ltm_nc)\n# attr(seasonal_sst_anomaly_gridded, \"data_steward\") &lt;- c(\n#   \"Kimberly Bastille &lt;kimberly.bastille@noaa.gov&gt;\")\n# attr(seasonal_sst_anomaly_gridded, \"plot_script\") &lt;- list(\n#   `ltl_MAB_shelf` = \"LTL_MAB.Rmd-shelf-seasonal-sst-anomaly-gridded.R\",\n#   `ltl_NE` = \"LTL_NE.Rmd-seasonal-sst-anomaly-gridded.R\")\n#\n# usethis::use_data(seasonal_sst_anomaly_gridded, overwrite = T)\n\n\n# #### Get Gridded Daily Max values for Marine Heatwaves\n#\n# mab_peak_hw &lt;- sst.2019[[210]] - summer.ltm## 7/28/2020\n# gb_peak_hw &lt;- sst.2019[[227]] - summer.ltm## 8/14/2020\n# gom_peak_hw &lt;- sst.2019[[214]] - summer.ltm## 8/2/2020\n#\n# rast_process_epu &lt;- function(r, epu){\n#   r &lt;- raster::stackApply(r, indices = rep(1,nlayers(r)),mean) #Find mean anomaly\n#   crs(r) &lt;- crs #Add SOE CRS\n#   ### Remove smoothing steps due to \"over smoothing'\n#   #r &lt;- disaggregate(r, 5) #interpolate step 1 - create higher res grid\n#   #r &lt;- focal(r, w=matrix(1,nrow=5,ncol=5), fun=mean,\n#   #           na.rm=TRUE, pad=TRUE) #interpolate step 2 - moving window\n#   r &lt;- as(r, \"SpatialPointsDataFrame\") #Convert to ggplot-able object\n#   r &lt;- as.data.frame(r)\n#   r &lt;- r %&gt;%\n#     reshape2::melt(id = c(\"y\",\"x\")) %&gt;%\n#     dplyr::rename(Latitude = y, Longitude = x) %&gt;%\n#     dplyr::select(-variable) %&gt;%\n#     dplyr::mutate(EPU = epu) %&gt;%\n#     dplyr::rename(Value = value)\n#\n#   return(r)\n# }\n#\n#\n# heatwave_peak_date &lt;-\n#   rbind(rast_process_epu(mab_peak_hw,epu = \"MAB\"),\n#         rast_process_epu(gb_peak_hw ,epu = \"GB\"),\n#         rast_process_epu(gom_peak_hw, epu = \"GOM\"))\n#\n# usethis::use_data(heatwave_peak_date, overwrite = T)\n\n\n\n\n\n3.1.6 6. Final Indicator Plotting\n\nNow that we have a new dataset for the GoM region all we have to do is run the previous code with the new data. I have saved the smaller GoM SST Anomaly dataset for convenience.\n\n\n\nCode\n# Set lat/lon window for maps\nxmin = -98.05 #CHANGED\nxmax = -80.43 #CHANGED\nymin = 17.40 #CHANGED\nymax = 31.46 #CHANGED\nxlims &lt;- c(xmin, xmax)\nylims &lt;- c(ymin, ymax)\n#Time series constants\nshade.alpha &lt;- 0.3\nshade.fill &lt;- \"lightgrey\"\nlwd &lt;- 1\npcex &lt;- 2\ntrend.alpha &lt;- 0.5\ntrend.size &lt;- 2\nhline.size &lt;- 1\nhline.alpha &lt;- 0.35\nhline.lty &lt;- \"dashed\"\nlabel.size &lt;- 5\nhjust.label &lt;- 1.5\nletter_size &lt;- 4\nfeeding.guilds &lt;- c(\"Apex Predator\",\"Piscivore\",\"Planktivore\",\"Benthivore\",\"Benthos\")\nx.shade.min &lt;- 2012\nx.shade.max &lt;- 2022\n#Function for custom ggplot facet labels\nlabel &lt;- function(variable,value){\n  return(facet_names[value])\n}\n\n\n#OUR REGION\nGoM_shp&lt;-rgdal::readOGR(\"C:/Users/brittanytroast/Dropbox/Work/Seaside Chats/Tutorials/Pics/Adapt NE SoE Indic/GoM Shapefile/GoM.shp\")\n\n\nOGR data source with driver: ESRI Shapefile \nSource: \"C:\\Users\\brittanytroast\\Dropbox\\Work\\Seaside Chats\\Tutorials\\Pics\\Adapt NE SoE Indic\\GoM Shapefile\\GoM.shp\", layer: \"GoM\"\nwith 1 features\nIt has 10 fields\n\n\nCode\nGoM_shp_sf&lt;-sf::st_as_sf(GoM_shp)\nGoM_shp_sf&lt;-GoM_shp_sf$geometry\n\n#Map line parameters\nmap.lwd &lt;- 0.4\n# Set lat/lon window for maps\nxmin = -98.05 #CHANGED\nxmax = -80.43 #CHANGED\nymin = 17.40 #CHANGED\nymax = 31.46 #CHANGED\nxlims &lt;- c(xmin, xmax)\nylims &lt;- c(ymin, ymax)\n\n# sst &lt;- ecodata::seasonal_sst_anomaly_gridded  #WHERE YOU LOAD THE DATA FROM ECODATA\nsst&lt;-read.csv(\"C:/Users/brittanytroast/Dropbox/Work/Seaside Chats/Tutorials/Data/SST Anom GoM.csv\") #GoM SST ANOM DAT FROM PREVIOUS FUNCTION\n\nsst$Season &lt;- factor(sst$Season, levels = c(\"Winter\",\n                                            \"Spring\",\n                                            \"Summer\",\n                                            \"Fall\"))\n\nsst&lt;- sst %&gt;% dplyr::mutate(Value = replace(Value, Value &gt; 5, 5))\nsst_map &lt;- \n  ggplot2::ggplot() +\n  ggplot2::geom_tile(data = sst, aes(x = Longitude, y = Latitude,fill = Value)) +\n  ggplot2::geom_sf(data = ecodata::coast, size = map.lwd) +\n  ggplot2::geom_sf(data = GoM_shp_sf, fill = \"transparent\", size = map.lwd) +\n  ggplot2::scale_fill_gradient2(name = \"Temp./nAnomaly (C)\",\n                       low = scales::muted(\"blue\"),\n                       mid = \"white\",\n                       high = scales::muted(\"red\"),\n                       limits = c(-5,5), \n                       labels = c(\"&lt;-5\", \"-2\", \"0\", \"2\", \"&gt;5\")) +\n  ggplot2::coord_sf(xlim = xlims, ylim = ylims) +\n  ggplot2::facet_wrap(Season~.) +\n  ecodata::theme_map() +\n  ggplot2::ggtitle(\"SST anomaly (2022)\") +\n  ggplot2::xlab(\"Longitude\") +\n  ggplot2::ylab(\"Latitude\") +\n  ggplot2::theme(panel.border = element_rect(colour = \"black\", fill=NA, size=0.75),\n        legend.key = element_blank(),\n        axis.title = element_text(size = 11),\n        strip.background = element_blank(),\n        strip.text=element_text(hjust=0),\n        axis.text = element_text(size = 8), \n        axis.title.y = element_text(angle = 90))+\n  ecodata::theme_title()\n\nsst_map \n\n\n\n\n\n\nThere you have it! The same indicator from the NE SoE report but adapted for the GoM."
  },
  {
    "objectID": "Tutorials/Adapt NE SoE Indicator.html#steps-now-that-we-have-more-information",
    "href": "Tutorials/Adapt NE SoE Indicator.html#steps-now-that-we-have-more-information",
    "title": "3  Adapt NE SoE Indicator",
    "section": "3.2 Steps now that we have more information",
    "text": "3.2 Steps now that we have more information\nAfter going through the previous process I am more informed and would use a different process if I wanted to adapt a new indicator. I will briefly describe these steps here.\n\n3.2.1 1. Search for indicators in the technical document\n\nI would start my search for indicators in the technical document. They have many more indicators than the reports do, as all indicators are not published. Plus I will get a good overview of the methods and datasets used in analysis. The technical report also has example figures for reference.\n\nNE SoE Technical Document\n\n\n3.2.2 2. Replicate Indicator\n\nI would replicate the indicator using the data provided by the ecodata package and using the associated code in the “chunk-scripts” folder in the ecodata repository.\nIf any argument was missing I would search for that argument in the repo (as I had to do previously with the setup code for x.min.shade)\n\n\n\n3.2.3 3. Check Data\n\nNext, I would check the extent of the data provided in the ecodata package and determine if it will work for my study area.\nIf the data do not cover my study area I would refer to the information provided by the technical data and follow links to the code that will grab and format the data for my preferred region.\n\n\n\n3.2.4 4. Plotting\n\nWith the new data I would triple check the code for the indicator and make sure there are not any geographic or other variables that need to be altered for my region.\nWith these changes hopefully I get the desired indicator for my region!"
  },
  {
    "objectID": "Tutorials/intro2spatial.html#introduction",
    "href": "Tutorials/intro2spatial.html#introduction",
    "title": "4  Intro to Spatial",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThis is a quick tutorial on manipulating spatial data in R. I will focus on 4 valuable packages: leaflet, terra, sf, and spatstat.\nFirst, some R housekeeping by loading needed libraries. I will be keeping this simple by using minimal packages (sorry tidyverse folks).\n\n\nCode\nlibrary(cmocean)\nlibrary(leaflet)\nlibrary(scales)\nlibrary(sf)\nlibrary(spatstat)\nlibrary(terra)\n\n\nNext, lets import some data. I have been working with BOEM’s oil rig dataset in the Gulf of Mexico. I removed rows that did not have lon/lat data and then turned it into an sf object using the st_as_sf function in the sf package.\n\n\nCode\nplatforms &lt;- read.csv('PlatStruc2.csv')\nplatforms &lt;- platforms[-which(is.na(platforms$Longitude)), ]\nplatforms_sf &lt;- st_as_sf(platforms,\n                        coords = c(\"Longitude\", \"Latitude\"),\n                        agr = \"identity\"\n                        )\n\nplot(platforms$Longitude, \n     platforms$Latitude,\n     pch = 16, \n     cex = .3, \n     col = alpha('gray20', .3),\n     asp = 1)"
  },
  {
    "objectID": "Tutorials/intro2spatial.html#lets-rasterize",
    "href": "Tutorials/intro2spatial.html#lets-rasterize",
    "title": "4  Intro to Spatial",
    "section": "4.2 Let’s rasterize",
    "text": "4.2 Let’s rasterize\nA important skill is to take data in tabular format (like from a csv) and turn into a raster. This is pretty easy with the rast and rasterize functions in the terra package. First, I need to create an empty raster to supply the values to; however, you cane take an existing raster and create a new one based upon the existing ones spatial dimensions and resolution. For this example, I am essentially creating a 2D histogram by counting, or rather using the length function, the number of oil rigs per raster cell. Any function could be used in the rasterize function such as mean, median, or even a user supplied function. Note, I am using the platforms that I turned into an sf object to to this. Then I check the coordinate reference system. Note that the rast function automatically supply a CRS, you can change this if needed.\n\n\nCode\nr &lt;- rast(\n  xmin = (-98), xmax = (-87),\n  ymin = (25), ymax = (31),\n  ncols = length(87:98) * 10, \n  nrows = length(25:31) * 10,\n  )\nplat_rast &lt;- rasterize(platforms_sf,\n                       r,\n                       \"Area.Code\",\n                       fun = length\n                       )\ncrs(plat_rast)\n\n\n[1] \"GEOGCRS[\\\"WGS 84 (CRS84)\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"unknown\\\"],\\n        AREA[\\\"World\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"OGC\\\",\\\"CRS84\\\"]]\"\n\n\nAlways plot your results to see if they make sense.\n\n\nCode\nplot(plat_rast, \n     col = cmocean('turbid')(100))"
  },
  {
    "objectID": "Tutorials/intro2spatial.html#customized-bathymetric-shapefile",
    "href": "Tutorials/intro2spatial.html#customized-bathymetric-shapefile",
    "title": "4  Intro to Spatial",
    "section": "4.3 Customized bathymetric shapefile",
    "text": "4.3 Customized bathymetric shapefile\nSay your interested in creating a shapefile that only encompasses the continential shelf out to 100 m depth. This is easy. First, import bathymetry select the depth range of interest and make it a polygon using the as.polygons function in the terra package. I downloaded a subsetted version of NOAA’s ETOPO bathymetry. There are other ways to get bathymetry like THREDDS, ERDDAP, or the marmap r package. Next, I will crop it to the extent of the oil rig data and add a little extra on each side to capture the full extent.\n\n\nCode\n# bathy &lt;- rast('etopo1.nc')\nbathy &lt;- marmap::getNOAA.bathy(lon1 = -98, lon2 = -87, lat1 = 25, lat2 = 31)\nbathy &lt;- marmap::as.raster(bathy)\n\nplot(bathy)\nmtext('NOAA ETOPO bathymetry')\n\n\n\n\n\nCode\n# ### isolate shelf\n# bathy[values(bathy$Band1) &gt; 0] &lt;- NA\n# bathy[values(bathy$Band1) &lt; (-100)] &lt;- NA\n\n### isolate shelf (new data) #BT EDIT\nbathy[bathy@data@values &gt; 0] &lt;- NA\nbathy[bathy@data@values &lt; (-100)] &lt;- NA\n\nplot(bathy,\n     main = 'Continential Shelf cutout')\n\n\n\n\n\nCode\n### set all values to 1; then make into shapefile\n# bathy[!is.na(bathy@data@values)] &lt;- 1\n\n\nbathy&lt;-rast(bathy) #BT EDIT\nbathy2 &lt;- terra::as.polygons(bathy) #BT EDIT\nbathy2 &lt;- terra::aggregate(bathy2) #BT EDIT\n\n\nbathy_c &lt;- crop(bathy2, \n                ext(min(platforms$Longitude,na.rm=T)-.1,\n                    max(platforms$Longitude,na.rm=T)+.1,\n                    min(platforms$Latitude,na.rm=T)-.1,\n                    max(platforms$Latitude,na.rm=T)+.1)\n)\n\nst_crs(platforms_sf) &lt;- crs(bathy_c)\nind &lt;- lengths(st_intersects(platforms_sf,st_as_sf(bathy_c)))&gt;0\n\nplot(bathy_c,\n     col = 3,\n     main = 'Continential Shelf shapefile cropped to oil rig location',\n     new=F)\nplot(platforms_sf$geometry[ind],\n     add=T,\n     pch = 16, \n     cex = .3, \n     col = alpha('purple', .4))"
  },
  {
    "objectID": "Tutorials/intro2spatial.html#random-and-uniform-points",
    "href": "Tutorials/intro2spatial.html#random-and-uniform-points",
    "title": "4  Intro to Spatial",
    "section": "4.4 Random and uniform points",
    "text": "4.4 Random and uniform points\nUsing the spatstat package we can make random points or uniformly spaced points within our shapefile.\n\n\nCode\ngom &lt;- st_as_sf(bathy_c)\nngom &lt;- st_transform(gom, 32615)\nwgom &lt;- as.owin(ngom)\n\n\n### random points\nrpts &lt;- runifpoint(1000,wgom)\nrpts2 &lt;- st_as_sf(data.frame(lon=rpts$x,lat=rpts$y),coords=c('lon','lat'))\nst_crs(rpts2) &lt;- 32615\nrpts2 &lt;- st_transform(rpts2, st_crs(gom))\nrpts2 &lt;- st_multipoint(cbind(unlist(rpts2$geometry)[seq(1,nrow(rpts2)*2,2)],\n                             unlist(rpts2$geometry)[seq(2,nrow(rpts2)*2,2)]))\n\n### uniformly spaced points\nupts &lt;- rsyst(wgom, nx = 100)\nupts2 &lt;- st_as_sf(data.frame(lon=upts$x,lat=upts$y),coords=c('lon','lat'))\nst_crs(upts2) &lt;- 32615\nupts2 &lt;- st_transform(upts2, st_crs(gom))\nupts2 &lt;- st_multipoint(cbind(unlist(upts2$geometry)[seq(1,nrow(upts2)*2,2)],\n                             unlist(upts2$geometry)[seq(2,nrow(upts2)*2,2)]))\n\n\nplot(bathy_c)\npoints(rpts2,pch=16,cex=.5,col=alpha(2,.7))\npoints(upts2,pch=16,cex=.5,col=alpha(4,.5))\nlegend(-89,27,c('random pts','uniform pts'),pch=16,col=c(2,4),bty='n')"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]